{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMD_Stochastic Gradient Descent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMMPI3NcfQyS/KpzbswH/ZU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seohyeon-jeong/basic_machinelearning/blob/main/SMD_Stochastic_Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1uNhsZl-hCs"
      },
      "source": [
        "### 확률적 경사 하강법Stochastic Gradient Descent\n",
        "`확률적 경사 하강법은 대표적인 점진적 학습 알고리즘의 일종  `\n",
        "\n",
        "> 점진적 학습 = 온라인 학습   \n",
        "앞서 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 훈련하는 방식 \n",
        "\n",
        "\n",
        "`경사 하강법의 모델을 훈련하는 것 = 가장 가파른 길을 조금씩 내려오는 과정  `\n",
        "\n",
        "> **확률적 경사 하강법** :  \n",
        "훈련 세트에서 랜덤하게 하나의 샘플을 선택(=확률적)해서 가파른 경사를 조금 내려가고, 그 다음 훈련 세트에서 랜덤하게 또 다른 샘플을 하나 선택해서 경사를 조금 내려가는 식으로 전체 샘플을 모두 사용할 때 까지 계속 -> 다 못내려오면 훈련 세트에 다시 샘플을 채워넣고, 다시 랜덤하게 하나의 샘플을 선택해 이어서 경사를 내려가는 방식으로 만족할만한 위치에 도달할 때까지 진행하는 방식.\n",
        "\n",
        "- 확률적 = 무작위하게, 랜덤하게 \n",
        "- 경사 = 기울기\n",
        "- 하강법 = 내려가는 방법  \n",
        "- 에포크epoch : 확률적 경사 하강법에서 훈련 세트를 한 번 모두 사용하는 과정  \n",
        "(일반적으로 경사 하강법은 수십, 수백 번 이상 에포크를 수행)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gha1553cYyDP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}