{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMD_Stochastic Gradient Descent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPQE6D+Mdn9yK1s6jVyIxBA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seohyeon-jeong/basic_machinelearning/blob/main/SMD_Stochastic_Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1uNhsZl-hCs"
      },
      "source": [
        "### 확률적 경사 하강법Stochastic Gradient Descent\n",
        "`확률적 경사 하강법은 대표적인 점진적 학습 알고리즘의 일종  `\n",
        "\n",
        "> 점진적 학습 = 온라인 학습   \n",
        "앞서 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 훈련하는 방식  \n",
        "-> 신경망 알고리즘에서 반드시 사용한다. \n",
        "\n",
        "`경사 하강법의 모델을 훈련하는 것 = 가장 가파른 길(=손실 함수)을 조금씩 내려오는 과정  `\n",
        "\n",
        "> **확률적 경사 하강법** :  \n",
        "훈련 세트에서 랜덤하게 하나의 샘플을 선택(=확률적)해서 가파른 경사를 조금 내려가고, 그 다음 훈련 세트에서 랜덤하게 또 다른 샘플을 하나 선택해서 경사를 조금 내려가는 식으로 전체 샘플을 모두 사용할 때 까지 계속 -> 다 못내려오면 훈련 세트에 다시 샘플을 채워넣고, 다시 랜덤하게 하나의 샘플을 선택해 이어서 경사를 내려가는 방식으로 만족할만한 위치에 도달할 때까지 진행하는 방식.\n",
        "\n",
        "- 확률적 = 무작위하게, 랜덤하게 \n",
        "- 경사 = 기울기\n",
        "- 하강법 = 내려가는 방법  \n",
        "- 에포크epoch : 확률적 경사 하강법에서 훈련 세트를 한 번 모두 사용하는 과정  \n",
        "(일반적으로 경사 하강법은 수십, 수백 번 이상 에포크를 수행)\n",
        "\n",
        "\n",
        "> **미니배치 경사 하강법minibatch gradient descent** :  \n",
        "1개씩이 아니라 무작위로 몇 개의 샘플을 선택해서 경사를 따라 내려가는 방법\n",
        "\n",
        "> **배치 경사 하강법batch gradient ** :\n",
        "한 번 경사로를 따라 이동하기 위해 전체 샘플 이용 (but 컴퓨터 자원을 너무 많이 사용해서 부하가 걸리기 쉽다)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GkXYgwNTkXU"
      },
      "source": [
        "####손실 함수 Loss Function =: 비용 함수cost function\n",
        "어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준  \n",
        "작을수록 좋지만, 어떤 값이 최소값인지 알지 못하기 때문에 가능한 한 많이 찾아보고 만족할만한 수준이면 산을 다 내려왔다고 인정해야 한다.  \n",
        "  ex. 분류에서의 손실 = 정답을 못맞추는 것\n",
        " - 손실 함수는 미분 가능해야 한다. (연속적인 곡선의 형태)\n",
        "\n",
        " - 손실 함수 : 샘플 하나에 대한 손실을 정의\n",
        " - 비용 함수 : 훈련 세트에 있는 모든 샘플에 대한 손실 함수의 합\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmsgdHuvZQ6u"
      },
      "source": [
        "#####로지스틱 손실 함수 \n",
        "로지스틱 회귀 모델에서 확률은 0~1 사이의 어떤 값도 될 수 있으므로 연속적이다.\n",
        "\n",
        "1. 양성클래스(타깃=1)일 때 손실은 -log(예측 확률)  \n",
        "예측 확률이 1에서 멀어질수록 손실은 아주 큰 양수가 된다.  \n",
        "ex. 예측 확률 0.9 , 타깃 1 -> -log(0.9)\n",
        "\n",
        "2. 음성 클래스(타깃=0)일 때 손실은 -log(1-예측 확률)  \n",
        "예측 확률이 0에서 멀어질수록 손실은 아주 큰 양수가 된다.  \n",
        "ex. 예측 확률 0.2, 타깃 0 -> -log(1-0.2) = -log(0.8)\n",
        "\n",
        "\n",
        "- 이진 크로스엔트로피 손실 함수binary cross-entropy loss function  \n",
        "= 로지스틱 손실 함수 (이진 분류)  -> 이 손실함수를 사용해 \"로지스틱 회귀 모델\"를 만든다  \n",
        "cf. 회귀는 손실함수로 평균 절대값 오차 또는 평균 제곱 오차를 이용\n",
        "> 평균 절대값 오차 = 타깃에서 예측을 뺀 절대값을 모든 샘플에 평균한 값  \n",
        "평균 제곱 오차 = 타깃에서 예측을 뺀 값을 제곱한 다음 모든 샘플에 평균한 값\n",
        "\n",
        "\n",
        "- 크로스 엔트로피 손실 함수cross-entropy loss function (다중 분류)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gha1553cYyDP"
      },
      "source": [
        "# SGDClassifier를 이용해 확률적 경사 하강법을 이용한 분류 모델 만들기\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}